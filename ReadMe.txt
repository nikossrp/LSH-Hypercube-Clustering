AM: sdi1600276, Νίκος Σερέπας
ΑΜ: sdi1700138, Κωνσταντίνος Σεμερτζάκης 

Compilation: make

execution nearest neighbor search: 
$./search –i <input file> –q <query file> –k <int> -L <int> -M <int> -probes <int> -ο <output file> -algorithm <LSH or Hypercube or Frechet> -metric <discrete or continuous | only for –algorithm Frechet> -delta <double>

execution clustering:
$./search –i <input file> –q <query file> –k <int> -L <int> -M <int> -probes <int> -ο <output file> -algorithm <LSH or Hypercube or Frechet> -metric <discrete or continuous | only for –algorithm Frechet> -delta <double>

Στο Makefile υπάρχουν εντολές για να τρέξει το πρόγραμμα απευθεία με καθένα αλγόριθμο ξεχωριστά με βάση τα datasets που δώθηκαν τα έχουμε βάλει στον φάκελο Datasets.
Στο φάκελο Ouputs, αυτό που κάναμε ήταν να τρέξουμε τον κάθε αλγόριθμο 2 φορές και να κρατήσουμε το καλύτερο αποτέλεσμα για τον καθένα απο αυτους, επιπλέον εκτυπώσαμε και τους χρόνουν που θέλει το κάθε query για καλύτερη συγκριση και τον αριθμό του query
Επίσης σε όσα tests κάναμε δεν υπήρχε κανένα memory leak.


Πειραματισμοί που κάναμε:
 Για τον lsh πειραματιστήκαμε με το w,  παρατηρίσαμε οτι όταν είναι της τάξης του είναι 500+ τότε φέρνει πολλές φορές ίδια αποτελέσματα με το brute foce αλλά αυξάνετε ο χρόνο για το αρχείο των input_small_id, παρο'λα αυτα οχι
οσο ο χρόνος του brute force. Οπότε φτιάξαμε την μέθοδο calculate_W() στην κλάση dataset, η μέθοδος αυτη παίρνει ενα υποσύνολο των δεδομένων (100 δυανίσματα/καμπύλες) και υπολογίζει την μέση απόσταση (L2) γα κάθε διάνυσμα μετα υπολογίζει 
την μέση απόσταση όλων των διανσιμάτων προσθετοντας όλες τις μέσες αποστάσεις και διαιρώντας τις με το πλήθος τους, το αποτέλεσμα αυτων είναι συνήθως κοντα στο 947  για το dataset που υπάρχει για την εξέταση. Εν τέλει το w το αφήσαμε 
κοντα  υποδιερέστερα του 947 ανάλογα τον αλγόριθμο. Αναλύτικότερα 

    Για το LSH/hypercube: W / 3 . 
    Για το lsh με continuous/discrete frechet : W/2 
    Για το clustering : W / 2

Όπου W υπολογίζεται απο την calculate_W(), δηλαδη η μέση απόσταση L2 των πρωτων 100 vectors

Το accuracy το υπολογίζουμε ως το αθροισμα των distanceTRUE διαιρώντας το με το αθροισμα των diistanceLSH. 
 Για το cluster με LSH/Hypercube πειραματιστήκαμε με την συνθήκη τερματισμού αρχικά είχαμε ως συνθήκη τερματισμού τα μισα κεντρα να μην κουνιουνται σε απόσταση μικρότερη του 1 σε σχεση με τα παλια κέντρα αλλα ο αλγόριθμος σταμάταγε
πολύ γρήγορα οπότε το αλλάξαμε και το κάναμε 0,5. Το brute force στον αλγόριθμο ξεκινάει όταν τουλάχιστον τα μισά cluster αυξάνοντας την ακτίνα δεν αρχικοποιούν κανένα δυάνισμα, δοκιμάσαμε να βάλουμε επιπλέον την συνθήκη οτι αν υπάρχει σύγκρουση 
μεταξυ δυο ακτινων στα clusters τοτε να ξεκινάει το brute force αλλά παρατηρήσαμε οτι το αποτέλεσμα για τα συγκεκριμένα datasets δεν αλλάζει.
Επίσης παρατηρήσαμε ότι αυξάνοντας το ε (πχ ε = 5) για το dataset της εξέτασης φέρνει καλύτερους χρόνουν ο Αλοριθμος Aiii αλλα δεν θέλαμε να είναι fixed οπότε το υπολογίζουμε με βάση την μέση αλλαγή που υπάρχει στην κάθε καμπυλη.
Τελος το συμπέρασμα που βγάλαμε είναι ότι τα βελτιστα αποτελέσματα για όλους τους αλγορίθμους εξαρτώνται απο:  w, δ, συνθήκη τερματισμου για το clustering και ε για το filtering για το ΚΑΘΕ dataset ξεχωριστά.



Αρχεία :   

Dataset.h / Dataset.cpp
    Για την κλάση του dataset χρησιμοποιήσαμε δυο δομές ένα vector και ένα unordered_map οπου είναι σαν hash table. Στις δομές αυτες κρατάμε δείκτες όπου είναι το item_id
και το αντοίστηχο διάνυσμα. Χρήσιμοποιήσαμε της δύο δομές ωστε να έχουμε ποιο γρήγορη πρόσβαση στα δεδομένα και να μην σπαταλάει χρόνο το πρόγραμμα για να πάρει εναν vector
απο το dataset. Συγκεκριμένα η συνάρτηση get_vector() οπου παίρνει ένα όρισμα string και έιναι το item_id χρησιμοποιέι το unordered_map για να κάνει search (O(1)) και
και να επιστρέψει τον vector που έχει το συγκεκριμένο item_id, ενώ η συνάρτηση get_i_vector() χρησιμοποιεί την δομή του vector και παίρνει το δυάνισμα στο συγκεκριμένο 
index κατι που μας βοήθισε πολύ για να διασχίσουμε ολα τα διανύσματα και να κάνουμε το brute force. Σπαταλάμε παραπάνω χώρο με τους δείκτες αλλα στην συγκεκριμένη
εργασία δεν μας ενδιαφέρει κιολας, καθως σημασία έχει να δούμε την απόδοση των αλγορίθμων. 


HashTable.h / HashTable.cpp
    Σε αυτα τα αρχεια υλοποιούνται τα hash tables για τον LSH και για τον Hypercube. Συγκεκριμένα έχουμε φτιάξει μια ιεραρχεία κλάσεων όπου η βασική κλάση υλοποιεί την
συνάρτηση κατακερματισμού (h = floor[(p v + t) / w]) που είναι κοινη στους δυο αλγορίθμους. Για το κάθε hash table υπολογίζονται τα δυανίσματα v1,...,vk και τα δυανίσματα t1,..,tk 
σε όλες τις συναρτήσεις g χρησιμοποιούνται τα ίδια δυανίσματα r1,...,rk. 
    Στην κλάση HashTableLSH υλοποιείτε το πως μπορεί ενα δυάνισμα να γίνει εισαγωγή υπολογίζοντας το ID και χρησιμοποιόντας της συναρτήσεις h για να κατακερματιστεί στο bucket
που του αντοιστηχεί. Στα περισσότερα buckets τα δυανίσματα έχουν το ίδιο ID δηλαδη είναι κοντά το ένα δυάνισμα με το άλλο.
    Για την κλάση HashTableHC, συγκεκριμένα για την συνάρτηση f(h) εχουμε χρησιμοποιήσει ενα unordered_map όπου κρατάει της τιμές της κάθε h_i και τις αντοιστιχεί με ομοιόμορφη τυχαία
κατανομή στο {0, 1}. Το key στο unordered_map είναι ενα string της μορφής "h_index/value" ωστε να ξέρουμε κάθε φορα ποιά συνάρτηση hi επέστρεψε το συγκεκριμένη τιμή (value).
πχ σε περίπτωση που το h1(p) = 5, τότε θα αποθηκευτεί ως key = "1/5".



LSH.h / LSH.cpp
    Είναι η δομή που διδάχτηκε στο μάθημα, συγκεκριμένα έχουμε χρησιμοποιήσει ενα vector απο HashTableLSH items οπου θα είναι τα L hash tables, το κάθε hash table έχει την δικιά του g
συνάρτηση κατακερματισμού όπως περιγράφτικε παραπάνω. 
    Για να βρούμε τους N πλησιεστερους γείτονες (μέθοδο findNN_LSH) παίρνουμε την τιμή της g που αντιστοιχεί στο query και παίρνουμε όλα τα vectors απο το hash table, το ίδιο κάνουμε 
και για τα L hash tables. Υστερα κρατάμε σε μια δομή τα vectors που έχουν ίδιο ID με το query,  αν ο αριθμος των vectors είναι μικρότερος απο τον αριθμό των πλησιεστερων γειτόνων που πρεπει
να εμφανίσουμε τότε κρατάμε όλα τα vectors και υπολογίζουμε αποστάσεις για το κάθε vector στο bucket απο το query ανεξαρτητα με το ID, σε διαφορετική περιπτωση βλέπουμε μόνο τα vectors με το
ίδιο ID.
    Για το range search (μέθοδος findNN_range_search) κάνουμε σχεδον την ίδια διαδικασία με την μέθοδο findNN_LSH χωρις να χρησιμοποιούμε το ID καθως δεν εχει σημασία γιατι ζητάμε δυανίσματα εντός
ακτίνας R απο το query.   
Στο Aii έχουμε L hash tables ενω στο Aiii 1 hash table.


Hypercube.h / Hypercube.cpp
    Είναι η δομή που υπόθηκε στο μάθημα, έχουμε ενα hash table της μορφης HashTableHC που υπόθηκε παραπάνω.
Για να βρούμε τους N πλησιεστερους γείτονες παίρνουμε την τιμή της fi για κάθε hi του query πηγαίνουμε στο bucket που του αντιστοιχεί και παίρνουμε Μ vectors αν είναι λιγότερα απο Μ τα δυανίσματα 
τότε πηγαίνουμε σε διπλανα buckets με hamming distance 1  ετσι ωστε να μαζέψουμε τα Μ διανύσματα. Το hamming distance αυξάνη γραμμικά δηαλδη θα ψάξουμε όλες τις κορυφές με hamming distance 1 μετα ολες
με hamming distance 2 κ.ο.κ 
Για το range search κάνουμε το ίδιο ακριβως απλα αντι να πάρουμε τους Ν πλησιεστερους γείτονες παίρνουμε όλα τα δυανίσματα εντος ακτίνας R.


Cluster.h / Cluster.cpp
    Αρχικά για τον K-means++ στην κλάση Dataset υπάρχει η μέθοδος k_means_plusplus οπου στην ουσια αυτο που κάνει είναι να επιλέγει αρχικά ενα τυχαίο κεντροιδη απο τα υπάρχοντα δυανίσματα στο dataset,
ύστερα ακολουθείτε ο αλγόριθμος στην διαφάνεια 46, δηλαδη υπολογίζουμε την ελάχιστη απόσταση απο τα υπάρχοντα κεντροιδη την υψώνουμε στο τετράγωνο και υπολογίζουμε το P(0),...,P(n-t) οπου αυτοι
οι αριθμοι αποθηκευονται σε εναν vector, μετα επιλεγουμε εναν αριθμό στο ευρος [P(0), P(n-t)] με τυχαια κατανομη και παίρνουμε το index του αμέσως επόμενου κεντρου που είναι κοντα αυτος αριθμός, αυτο γίνεται
μέχρι να συμπληρωθεί ο αριθμός των κεντρων που ζητείτε. 
    Η μέθοδος update_centroids υπολογίζει το μέσω τον διανισμάτων σε κάθε cluster ξεχωριστα και επιστρέφει τα νεα κεντρα για τα clusters.
    Η μέθοδος Lloyds φτιάχνει τα clusters με τον κλασικο τρόπο χρησιμοποιώντας brute force.
    Η μέθοδο Range_Search παίρνει σας όρισμα την μέθοδο που θέλουμε να ακολουθήσουμε ωστε να φτιαχτούν τα clusters (LSH or Hypercube) και ακολουθείτε ο αλγόριθμος των διαφανειων στην σελ. 50, το μόνο που αλλάζει
    στους δύο αλγοριθμους είναι ο τρόπος που μπαίνουμε στα/στο bucket/s.
    Η μέθοδος silhouette είναι η υλοποιήση του αλγορίθμου στην σελ. 53


Grid.cpp 
    Υλοποιούμε το grid που σηζητίθηκε στην θεωρεία, χρησιμοποιούνται οι τύποι απο τις διαφάνειες για snapping/minima-maxima/remove-consecutive-duplicates. Για padding χρησιμοποιούμε την μεγαλύτερη τιμή που υπάρχει 
μεσα στο dataset αυξάνοντας την κατα 1000.


main_search.cpp / main_cluster.cpp
    Είναι οι main συναρτησεις για την επίδειξη των αλγορίθμων, στο output εμφανίζεται το accuracy και το πόσο γρηγορότερος είναι ο αλγόριθμος που χρησιμοποιούμε σε σχέση με το brute force. Ενω στα παραγόμενα αρχεια
εμφανίζονται τα αποτελέσματα που ζητούνται στην εργασία.





github: https://github.com/nikossrp/Project_Emiris
